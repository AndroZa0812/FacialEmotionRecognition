{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881a4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Dropout,Activation,Conv2D,MaxPooling2D,BatchNormalization,Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a4d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "angry\n",
      "disgust\n",
      "fear\n",
      "happy\n",
      "neutral\n",
      "sad\n",
      "surprise\n",
      "validation\n",
      "angry\n",
      "disgust\n",
      "fear\n",
      "happy\n",
      "neutral\n",
      "sad\n",
      "surprise\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[50], [32], [15], [11], [12], [10], [10], [8...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[29], [24], [29], [11], [15], [22], [26], [1...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0], [0], [0], [0], [0], [0], [0], [0], [0],...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[155], [138], [98], [112], [94], [97], [111]...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[211], [215], [220], [221], [219], [216], [2...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose  \\\n",
       "0  [[[50], [32], [15], [11], [12], [10], [10], [8...  angry       T   \n",
       "1  [[[29], [24], [29], [11], [15], [22], [26], [1...  angry       T   \n",
       "2  [[[0], [0], [0], [0], [0], [0], [0], [0], [0],...  angry       T   \n",
       "3  [[[155], [138], [98], [112], [94], [97], [111]...  angry       T   \n",
       "4  [[[211], [215], [220], [221], [219], [216], [2...  angry       T   \n",
       "\n",
       "                                          image_path  \n",
       "0  ./FinalProjData/fer2013/train/angry/Training_1...  \n",
       "1  ./FinalProjData/fer2013/train/angry/Training_1...  \n",
       "2  ./FinalProjData/fer2013/train/angry/Training_1...  \n",
       "3  ./FinalProjData/fer2013/train/angry/Training_1...  \n",
       "4  ./FinalProjData/fer2013/train/angry/Training_1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_path = \"./FinalProjData\"\n",
    "\n",
    "int2emotions = {0:'angry',1:'fear',2:'happy',3:'neutral',4:'sad',5:'surprise',6:'disgust'}\n",
    "emotions2int = {'angry':0,'fear':1,'happy':2,'neutral':3,'sad':4,'surprise':5,'disgust':6}\n",
    "\n",
    "dic = {'images':[], 'labels':[], 'purpose':[], 'image_path':[]}\n",
    "\n",
    "for d in os.listdir(f'{files_path}/fer2013/'):\n",
    "    print(d)\n",
    "    for emotion in os.listdir(f'{files_path}/fer2013/{d}'):\n",
    "        print(emotion)\n",
    "        for i in os.listdir(f'{files_path}/fer2013/{d}/{emotion}'):\n",
    "            img = cv2.imread(f'{files_path}/fer2013/{d}/{emotion}/{i}',0)\n",
    "            img = img.reshape(48,48,1)\n",
    "\n",
    "            dic['images'].append(img)\n",
    "            dic['labels'].append(emotion)\n",
    "            dic['image_path'].append(f'{files_path}/fer2013/{d}/{emotion}/{i}')\n",
    "\n",
    "            if d=='train':\n",
    "                dic['purpose'].append('T')\n",
    "            else:\n",
    "                dic['purpose'].append('V')\n",
    "\n",
    "df = pd.DataFrame(dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbe6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df['purpose']=='T']\n",
    "val_data = df[df['purpose']=='V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce491c-e097-448e-a454-ccb7a0fc9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72b519-a2c1-4d8f-9c79-29b04a7763e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459c412-f61a-476a-bae9-3c0dead0d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10d509-e151-465c-a29d-2f75ad87f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a229368d-8e12-4adc-8f18-3ab84fb67875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[222], [211], [219], [221], [222], [224], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/happy/Training_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[255], [255], [255], [255], [255], [251], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/happy/Training_4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[121], [122], [137], [139], [141], [150], [1...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/surprise/Trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[8], [18], [26], [24], [22], [15], [12], [25...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/neutral/Training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[82], [89], [87], [83], [78], [75], [82], [1...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "      <td>./FinalProjData/fer2013/train/angry/Training_2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images    labels purpose  \\\n",
       "0  [[[222], [211], [219], [221], [222], [224], [2...     happy       T   \n",
       "1  [[[255], [255], [255], [255], [255], [251], [2...     happy       T   \n",
       "2  [[[121], [122], [137], [139], [141], [150], [1...  surprise       T   \n",
       "3  [[[8], [18], [26], [24], [22], [15], [12], [25...   neutral       T   \n",
       "4  [[[82], [89], [87], [83], [78], [75], [82], [1...     angry       T   \n",
       "\n",
       "                                          image_path  \n",
       "0  ./FinalProjData/fer2013/train/happy/Training_7...  \n",
       "1  ./FinalProjData/fer2013/train/happy/Training_4...  \n",
       "2  ./FinalProjData/fer2013/train/surprise/Trainin...  \n",
       "3  ./FinalProjData/fer2013/train/neutral/Training...  \n",
       "4  ./FinalProjData/fer2013/train/angry/Training_2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_df = train_data[train_data['labels']=='happy'].sample(n=3171)\n",
    "neutral_df = train_data[train_data['labels']=='neutral'].sample(n=3171)\n",
    "sad_df = train_data[train_data['labels']=='sad'].sample(n=3171)\n",
    "fear_df = train_data[train_data['labels']=='fear'].sample(n=3171)\n",
    "angry_df = train_data[train_data['labels']=='angry'].sample(n=3171)\n",
    "surprise_df = train_data[train_data['labels']=='surprise'].sample(n=3171)\n",
    "disgust_df = train_data[train_data['labels']=='disgust'].sample(n=436)\n",
    "\n",
    "train_data = pd.concat([happy_df,neutral_df,sad_df,fear_df,angry_df,surprise_df, disgust_df])\n",
    "\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop('index',inplace=True,axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4cbb3c-fcbc-4224-b7a5-fce126d15964",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6321a4-5a2a-46b2-b6ad-5704b20f9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23d213bf-1180-43cf-8fbe-ab8af7d34d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "classes = 7\n",
    "rows,columns=48,48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac195c-72f8-4029-a6b8-adf161a518bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee065a-2788-48fa-a7f3-12a9bdee7267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535adf5f-e2a6-4d62-9381-14272a58b4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to numpy array\n",
    "train_labels = list(train_data['labels'].replace(emotions2int))\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "val_labels = list(val_data['labels'].replace(emotions2int))\n",
    "val_labels = to_categorical(val_labels)\n",
    "\n",
    "train_data = list(train_data['images'])\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "val_data = list(val_data['images'])\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4c760-99de-448f-85eb-4410b472f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e85ce1-2cb1-4585-a6e8-c8462ec377f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d63c744-c17d-4630-897a-d7f066bc6297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 48, 48, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 24, 24, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 12, 12, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 6, 6, 512)        2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,915,207\n",
      "Trainable params: 5,910,471\n",
      "Non-trainable params: 4,736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Block\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second Block\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third Block\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fourth Block\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fifth Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Sixth Block\n",
    "model.add(Dense(128,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Seventh Block\n",
    "model.add(Dense(64,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Eighth Block\n",
    "model.add(Dense(classes,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0da780ef-e82f-4c98-a736-f800c526abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(f'{files_path}\\model\\\\6_class_emotion_detector_V2.h5',\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                             verbose=1,\n",
    "                             min_delta=0,\n",
    "                             monitor='val_loss',\n",
    "                             restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n",
    "\n",
    "model.compile(metrics=['accuracy'],\n",
    "             optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b713a5e3-786d-4473-a8f0-1e2791b488a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 28273\n",
    "validation_samples = 3534\n",
    "batch_size = 64\n",
    "epochs=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2c696-b7ba-4d6a-a1ff-37059d38fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD WAY\n",
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    validation_data=(val_data,val_labels),\n",
    "                    validation_steps=validation_samples//batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f679db6-ba28-4543-b263-d0d033d56e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19462 validated image filenames belonging to 7 classes.\n",
      "Found 7178 validated image filenames belonging to 7 classes.\n",
      "Epoch 1/30\n",
      "305/441 [===================>..........] - ETA: 4:31 - loss: 2.4222 - accuracy: 0.1669WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 13230 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 3.29673, saving model to ./FinalProjData\\model\\6_class_emotion_detector_V2.h5\n",
      "441/441 [==============================] - 640s 1s/step - loss: 2.4222 - accuracy: 0.1669 - val_loss: 3.2967 - val_accuracy: 0.1006\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation setup for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Data generator for validation data (No augmentation)\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Create generators to read images from dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    directory=None,  # Directory is None since paths are absolute\n",
    "    x_col='image_path',\n",
    "    y_col='labels',\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_data,\n",
    "    directory=None,\n",
    "    x_col='image_path',\n",
    "    y_col='labels',\n",
    "    target_size=(48, 48),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "# Training the model using generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_samples // batch_size,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdb90f-53fe-4fef-8fb0-7824603ea7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "int2emotions = {0:'angry',1:'fear',2:'happy',3:'neutral',4:'sad',5:'surprise',6:'disgust'}\n",
    "model = load_model(f'{files_path}\\model\\\\6_class_emotion_detector_V2.h5')\n",
    "\n",
    "classifier = cv2.CascadeClassifier(f'{files_path}\\Haarcascades\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face(frame):\n",
    "    faces=classifier.detectMultiScale(frame,1.3,4)\n",
    "    if len(faces) == 0:\n",
    "        return frame\n",
    "    for x,y,w,h in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(172,42,251),2)\n",
    "        face = frame[y:y+h,x:x+w]\n",
    "        face = cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face,(48,48))\n",
    "        face = face.reshape(1,48,48,1)\n",
    "        cv2.putText(frame,text=int2emotions[np.argmax(model.predict(face))],\n",
    "                    org=(x,y-15),fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(106,40,243),thickness=2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6330dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    ret,frame= cap.read()\n",
    "    if ret==True:\n",
    "        cv2.imshow('emotion_detector',detect_face(frame))\n",
    "        if cv2.waitKey(1)==27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca19945-bb91-4a32-9a7a-f01f807a9743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
